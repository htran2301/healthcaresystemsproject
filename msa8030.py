# -*- coding: utf-8 -*-
"""MSA8030.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U2PWECWdrGTRCquSVXLBATWoAQV6rq9J

####Group project MSA 8030
####Team members:
####Meher Fatima
####Tran Le
####Maria Malhotra
####Hyun Cheon

### Data Engineer section
"""

import pandas as pd
import numpy as np

# 1. Load data
df = pd.read_csv("Impact_of_Remote_Work_on_Mental_Health.csv")

# 2. Clean column names
df.columns = (
    df.columns
      .str.strip()
      .str.replace(" ", "_")
      .str.lower()
)

# 3. Basic structure check
print("Shape:", df.shape)
print("\nInfo:")
df.info()
print("\nDescribe (numeric):")
print(df.describe())

# 4. Handle duplicates
print("\nNumber of duplicate rows before dropping:", df.duplicated().sum())
df = df.drop_duplicates().reset_index(drop=True)
print("Shape after dropping duplicates:", df.shape)

# 5. Missing values overview
print("\nMissing values per column:")
print(df.isna().sum())

# 6. Enforce numeric types first
critical_numeric = ["age", "hours_worked_per_week", "number_of_virtual_meetings"]
for col in critical_numeric:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors="coerce")

# 7. Drop rows with missing in critical numeric columns
df = df.dropna(subset=[c for c in critical_numeric if c in df.columns])

print("\nMissing values after handling key columns:")
print(df.isna().sum())


# 8. Final check
print("\nFinal info after cleaning:")
df.info()
print("\n Summary Statistics")
df.describe()

# DATA ANALYST SECTION


import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# 1. Summary Statistics

print(" Summary Statistics ")
print(df.describe(include="all"))

print("\n Missing Values")
print(df.isna().sum())

print("\n Unique Values ")
for col in df.columns:
    print(col, ":", df[col].nunique())

# 2. Distribution of Key Variables

plt.figure(figsize=(10,5))
sns.histplot(df["hours_worked_per_week"], bins=30, kde=True)
plt.title("Distribution of Weekly Work Hours")
plt.show()

plt.figure(figsize=(10,5))
sns.countplot(
    data=df,
    x="mental_health_condition",
    order=["Burnout", "Depression", "Anxiety", "Unknown"]
)
plt.title("Mental Health Condition Distribution")
plt.show()




# 3. Work Location Analysis

plt.figure(figsize=(10,6))
sns.boxplot(data=df, x="work_location", y="hours_worked_per_week")
plt.title("Hours Worked per Week by Work Location")
plt.show()



# 4. Outlier Detection (Boxplot)

plt.figure(figsize=(10,5))
sns.boxplot(x=df["hours_worked_per_week"])
plt.title("Outlier Check: Weekly Work Hours")
plt.xlabel("Hours Worked per Week")
plt.show()

plt.figure(figsize=(10,5))
sns.histplot(df["hours_worked_per_week"], bins=30, kde=True)
plt.title("Distribution of Weekly Work Hours")
plt.xlabel("Hours Worked per Week")
plt.ylabel("Count")
plt.show()

plt.figure(figsize=(10,5))
sns.countplot(
    data=df,
    x="mental_health_condition",
    order=["Burnout", "Depression", "Anxiety", "Unknown"] #later we will not use unknown
)
plt.title("Mental Health Condition Distribution")
plt.xlabel("Mental Health Condition")
plt.ylabel("Count")
plt.show()

"""###Visualization"""

import warnings
warnings.filterwarnings("ignore", category=FutureWarning) #this one is used to ignore the future warning we might have

# 0. SETUP: Libraries & Color Palettes


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Make Seaborn + Matplotlib look clean
sns.set_theme(style="whitegrid")
plt.rcParams["figure.figsize"] = (10, 6)
plt.rcParams["axes.titlesize"] = 16
plt.rcParams["axes.labelsize"] = 13

# --------- Color Palettes (based on color theory) ---------
# Emotional palette for mental health conditions
mental_palette = {
    "Burnout": "#d73027",     # strong red = most severe
    "Depression": "#fc8d59",  # orange-red = negative emotion
    "Anxiety": "#fee08b",     # yellow = tension / warning

}

# Traffic-light palette for stress
stress_palette = {
    "Low": "#1a9850",         # green
    "Medium": "#fee08b",      # yellow
    "High": "#d73027",        # red
}

# Diverging palette for productivity change
productivity_palette = {
    "Decrease": "#b2182b",    # red
    "No Change": "#999999",   # grey
    "Increase": "#1a9850",    # green
}


# Distinct region palette (if you need it later)
region_colors = sns.color_palette("Set2", 6)




# 2. PERFORMANCE: Productivity Change × Mental Health Condition
#    (Grouped Bar Chart)


plt.figure(figsize=(10, 6))
sns.countplot(
    data=df,
    x="mental_health_condition",
    hue="productivity_change",
    order=["Burnout", "Depression", "Anxiety"],
    hue_order=["Decrease", "No Change", "Increase"],
    palette=productivity_palette
)
plt.title("Productivity Change Across Mental Health Conditions")
plt.xlabel("Mental Health Condition")
plt.ylabel("Number of Employees")
plt.legend(title="Productivity Change", bbox_to_anchor=(1.02, 1), loc="upper left")
plt.tight_layout()
plt.show()

# 3. REGION: Mental Health Condition by Region (Heatmap)

# filter out Unknown
mask = df["mental_health_condition"] != "Unknown"
df_no_unknown = df[mask]

# crosstab on filtered data
region_cond = pd.crosstab(
    df_no_unknown["region"],
    df_no_unknown["mental_health_condition"]
)

plt.figure(figsize=(10, 6))
sns.heatmap(
    region_cond,
    annot=True,
    fmt="d",
    cmap="Blues"
)
plt.title("Counts of Mental Health Conditions by Region")
plt.xlabel("Mental Health Condition")
plt.ylabel("Region")
plt.tight_layout()
plt.show()

"""##**Interpretation**
###Figure: "Productivity Change Across Mental Health Conditions"
####There is a marked decrease in productivity in respondents who have depression, and a slight decrease in productivity in those who have experienced burnout. Note that we do not know if the mental health conditions are pre-existing or not.
###Figure:" Counts of Mental Health Conditions by Region"
####Our data says that depression is most prevalent in South America, while both burnout and anxiety are most prevalent in Asia. Burnout has the lowest prevalence in South America. This heatmap uses the cool color blue in a mix of juxtaposed shades of different saturation to emphasize the Gestalt principles of proximity and symmetry.



"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Re-load df and apply column name cleaning as done in Data Engineer section
df = pd.read_csv("Impact_of_Remote_Work_on_Mental_Health.csv")
df.columns = (
    df.columns
      .str.strip()
      .str.replace(" ", "_")
      .str.lower()
)

plt.figure(figsize=(12, 6))

ax = sns.violinplot(
    data=df,
    x="work_location",
    y="hours_worked_per_week", # Corrected to snake_case
    hue="industry", # Corrected to snake_case
    cut=0
)

plt.title("Hours Worked per Week by Work Location and Industry")
plt.xlabel("Work Location")
plt.ylabel("Hours Worked per Week")

# Move legend outside
handles, labels = ax.get_legend_handles_labels()
ax.legend(
    handles,
    labels,
    title="Industry",
    bbox_to_anchor=(1.02, 1),   # shift a bit to the right
    loc="upper left",
    borderaxespad=0
)

plt.tight_layout()
plt.show()

"""###**Interpretation**
The violins show a significant number of employees across all seven industries putting in 50 to 55-hour work-weeks, with the highest ones being Consultants from all three work arrangements (Hybrid, Onsite and Remote). The least density belongs to the Hybrid Finance professionals who work at an average of 35 hours, followed by hybrid IT workers at an average of 32 hours. Hybrid retail industry employees worked the lowest median hours (36) while hybrid Finance professionals worked the highest median hours (43). Remote and onsite workers across all seven industries have remarkably similar and almost identical patterns, most likely both following a normal distribution, suggesting the big picture lack of difference between remote and onsite arrangements, dismantling the myth of remote workers possibly not working as much as onsite workers. This violin plot utilizes Gestalt principles similarity, proximity, symmetry and closure through cool harmonic colors with medium saturation and subtle hues.

"""

cat_cols = df.select_dtypes(include=["object", "category"]).columns

for col in cat_cols:
    print(col, ":", df[col].nunique(), "categories")

for col in cat_cols:
    print(f"\n{col} categories:")
    print(df[col].unique())

# Create split dfs for each region MAM 112525 3:56 PST

# MAM 112825 14:15 PST added Africa

import pandas as pd

df=pd.read_csv("Impact_of_Remote_Work_on_Mental_Health.csv")

# Specify the column
column_to_split_by = 'Region'

# Create a dictionary of DataFrames, where each key is a unique value
# from the specified column, and the corresponding value is a DataFrame
# containing only the rows where that column has that specific value.
separated_dfs = {value: group for value, group in df.groupby(column_to_split_by)}

# You can also iterate through the dictionary to process each DataFrame
#for category_value, category_df in separated_dfs.items():
    #print(f"DataFrame for {category_value}:",category_df)

df_Asia = separated_dfs['Asia']
df_Europe = separated_dfs['Europe']
df_South_America = separated_dfs['South America']
df_North_America = separated_dfs['North America']
df_Oceania = separated_dfs['Oceania']
df_Africa = separated_dfs['Africa']

## Create smaller dfs based on job from the regional dfs MAM 112825 15:30 PST

import pandas as pd

df = df_North_America

# Specify the column you want to use for splitting
column_to_split_by = 'Job_Role'

separated_dfs = {value: group for value, group in df.groupby(column_to_split_by)}

df_NA_HR = separated_dfs['HR']
df_NA_Sales = separated_dfs['Sales']
df_NA_Project_Manager = separated_dfs['Project Manager']
df_NA_Data_Scientist = separated_dfs['Data Scientist']
df_NA_Software_Engineer = separated_dfs['Software Engineer']
df_NA_Designer = separated_dfs['Designer']
df_NA_Marketing = separated_dfs['Marketing']

"""**Interpretation**: In North America, Marketing professionals do not work as many hours as Data Scientists, Sales and Software Engineers. HR employees in their 50s are significantly working less hours (20-35 hours per week). Sales professionals in the 25-30 and 50-60 age groups are the hardest-working segments, with both groups significantly rendering 50 to 55-hour work weeks. A significant segment of the 50-60 age group in Sales, though, are working only 20-35 hours per week.This same exact trend of both a young age group and 50-60 age group working highest hours shows up in the Project Manager professionals. Data Scientists aged 40-60 work more hours than the younger segments. On the other hand, Designers in their 40s put in the highest work hours.

The general trend in North America though, points out that the older professionals generally put in more hours than their younge counterparts.
"""

# Correlogram MAM 112425 4:01 PST

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

#df=pd.read_csv("Impact_of_Remote_Work_on_Mental_Health.csv")
df = df_NA_Data_Scientist

# Custom the inside plot: options are: “scatter” | “reg” | “resid” | “kde” | “hex”
sns.jointplot(x=df["Age"], y=df["Hours_Worked_Per_Week"], kind='hex')
sns.jointplot(x=df["Age"], y=df["Hours_Worked_Per_Week"], kind='kde')

plt.show()

# Violin plot MAM 112425 4:02 PST
## Interpretation: In North America, professionals are working significantly longer hours than normal, going to up to 70
##     hours per week, which is hardly contributing to increase in productivity and instead could be associated with
##.    occurrence of mental health conditions. The highest effect can be seen with those who reported anxiety and working
##.    40-hour weeks, who reported increase in productivity.


plt.figure(figsize=(10, 6))

ax = sns.violinplot(
    x='Mental_Health_Condition',
    y='Hours_Worked_Per_Week',
    hue='Productivity_Change',
    palette='deep',
    data=df
)

# Title and axis labels
plt.title('Violin Plot of Mental Health Condition vs Hours Worked Per Week')
plt.xlabel('Mental Health Condition'
)
plt.ylabel('Hours Worked Per Week')

#Legend
handles, labels = ax.get_legend_handles_labels()
ax.legend(
    handles, labels,
    title='Productivity Change',
    bbox_to_anchor=(1.02, 1),
    loc='upper left',
    borderaxespad=0
)

plt.tight_layout()
plt.show()

"""**Interpretation**: In North America, professionals are working significantly longer hours than normal, going tup to 70
hours per week, which is hardly contributing to increase in productivity and instead could be associated with occurrence of mental health conditions.The highest effect can be seen with those who reported anxiety and working 40-hour weeks, who reported increase in productivity.

"""

## Violin Plot MAM 112525
## Interpretation: For North American Data Scientists, work-week hours are high topping at around 78, with the highest
##       occurrence on people who reported burnout while working 40-60 hours per week, yet reporting no change in their
##.      productivity. There is also a significant number of professionals with depression who worked 30-50 hours per
##.      week but reported decreased productivity. Anxiety is highest on professioanls who worked 35-50 hours but reported
##.      increased productivity

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df=df_NA_Data_Scientist

# Create the violin plot
# 'x' typically represents a categorical variable (groups)
# 'y' typically represents a continuous variable (distribution)
plt.figure(figsize=(10, 6)) # Adjust figure size as needed
ax = sns.violinplot(x='Mental_Health_Condition', y='Hours_Worked_Per_Week',hue='Productivity_Change',palette='viridis',data=df)

# Add title and labels for clarity
plt.title('Violin Plot of Mental Health Condition vs Hours Worked Per Week')
plt.xlabel('Mental Health Condition')
plt.ylabel('Hours Worked Per Week')

# Legend
handles, labels = ax.get_legend_handles_labels()
ax.legend(
    handles, labels,
    title='Productivity Change',
    bbox_to_anchor=(1.02, 1),
    loc='upper left',
    borderaxespad=0
)

# Display the plot
plt.tight_layout()
plt.show()

"""**Interpretation**: For North American Data Scientists, work-week hours are high topping at around 78, with the highest
occurrence on people who reported burnout while working 40-60 hours per week, yet reporting no change in their
productivity. There is also a significant number of professionals with depression who worked 30-50 hours per
week but reported decreased productivity. Anxiety is highest on professioanls who worked 35-50 hours but reported
increased productivity
"""

# Strip plot MAM 112425

## Interpretation: In North America, there is no recognizable pattern that implies that one work location is better
## than the other. Similarly, work location does not seem to be a factor in reducing or increasing stress
## levels of workers.

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = df_North_America

# Set the style of the plot
sns.set_style("dark")

# Create the strip plot
# x: The categorical variable for the x-axis
# y: The numerical variable for the y-axis
# data: The DataFrame containing the data
# jitter: Adds a small amount of random noise to the x-axis to prevent overlapping points, making them more visible.

sns.stripplot(x="Work_Location", y="Hours_Worked_Per_Week", hue='Stress_Level',palette='husl',data=df, jitter=0.1, s=5)

# Add a title to the plot
plt.title("Strip Plot of Work Location vs. Number of Hours Worked per Week")
plt.xlabel("Work Location")
plt.ylabel("Hours Worked per Week")
plt.legend(title="Stress Level", loc="upper left", bbox_to_anchor=(1, 1))

# Display the plot
plt.show()

"""
**Interpretation**: In North America, there is no recognizable pattern that implies that one work location is better
than the other. Similarly, work location does not seem to be a factor in reducing or increasing stress
levels of workers.

"""

# Mosaic plot MAM 112625

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.graphics.mosaicplot import mosaic



# create a contingency table (counts)
counts_table = pd.crosstab(
    index=[df['Job_Role'], df['Stress_Level']],
    columns=df['Productivity_Change']
)

# sort columns for consistent ordering
counts_table = counts_table[['Decrease', 'No Change', 'Increase']]

# calculate the percentages for each job/stress combination
# We want percentages relative to the total count *within* that Job/Stress combination
percentages_table = counts_table.div(counts_table.sum(axis=1), axis=0) * 100

def custom_labelizer(key):
    """
    Looks up the percentage for a given tile key in the percentages_table.
    """
    try:
        # Key format: (Job, Stress Level, Productivity Change Type)
        job, stress, change_type = key

        # Access the pre-calculated percentage and format it
        percent_value = percentages_table.loc[(job, stress), change_type]

        # Only label if there is data
        if pd.notna(percent_value) and percent_value > 0:
            return f"{percent_value:.1f}%"
        else:
            return "" # Return empty string for zero values or missing data

    except KeyError:
        # Handle cases where a key might not exist in the original data structure
        return ""

# Create a mosaic plot
# The 'index' argument specifies the order of the categorical variables
# The 'title' argument sets the plot title
fig, ax = plt.subplots(figsize=(20, 10))

plt.rcParams["figure.figsize"] = [20, 10]

fig, rects = mosaic(
    df,
    index=['Job_Role', 'Stress_Level', 'Productivity_Change'],
    ax=ax,
    labelizer=custom_labelizer,
    gap=0.01,
)

fig.suptitle('Productivity Change After Moving to Remote Work by Job and Stress Level', fontsize=18, fontweight='bold')
ax.set_xlabel('Job', fontsize=16)
ax.set_ylabel('Stress Level', fontsize=16)

# Increase *Inner Tile Labels* Size
# Loop through all Text objects added to the Axes by the mosaic function
for text_obj in ax.texts:
    text_obj.set_fontsize(12) # Set your desired size here
    text_obj.set_color('white')

# 4. Increase Tick Label Sizes (Bottom labels 'Eng', 'Sales', etc.)
plt.xticks(rotation=20, ha='left', fontsize=13)
plt.yticks(fontsize=13)

# Display the plot
plt.show()

"""**Interpretation**: Marketing, Project Managers and HR employees had the highest stress levels coinciding with decreased
productivity levels. Marketing, Sales and Data Scientists have the lowest stress levels coinciding with increased
productivity levels. Data Scientists have the highest increase in productivity levels among everyone reporting all
levels of stress (low, medium, high).The highest decrease in productivity was reported by Designers, Project Managers
and Sales.
"""

# Sunburst with per-slice percentages and larger figure for visibility
# MAM 112625

import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import colorsys

# Read data
df = pd.read_csv("Impact_of_Remote_Work_on_Mental_Health.csv")

# Palette for top-level categories
custom_sequence = ['#42f5d4', '#f5ec42', '#427b5F', '#FFe6e6', '#FFecb3']

# helper color functions
def hex_to_rgb(h):
    h = h.lstrip('#')
    return tuple(int(h[i:i+2], 16) for i in (0, 2, 4))

def rgb_to_hex(rgb):
    return '#%02x%02x%02x' % tuple(max(0, min(255, int(x))) for x in rgb)

def adjust_lightness(hex_color, factor):
    # factor >1 => lighter, <1 => darker
    r, g, b = hex_to_rgb(hex_color)
    r_f, g_f, b_f = [x / 255.0 for x in (r, g, b)]
    h, l, s = colorsys.rgb_to_hls(r_f, g_f, b_f)
    l = max(0, min(1, l * factor))
    r2, g2, b2 = colorsys.hls_to_rgb(h, l, s)
    return rgb_to_hex((r2 * 255, g2 * 255, b2 * 255))

# Aggregate values by hierarchy so we have clean totals
# Count respondents for each combination (we want percentages of people)
grouped = (
    df
    .groupby(['Job_Role', 'Mental_Health_Condition', 'Productivity_Change'], dropna=False)
    .size()
    .reset_index(name='count')
)

# Build tree lists
labels = []
ids = []
parents = []
values = []
colors = []

job_roles = list(grouped['Job_Role'].dropna().unique())
palette = custom_sequence if len(custom_sequence) >= len(job_roles) else px.colors.qualitative.Plotly
base_color_map = {}
for i, job in enumerate(job_roles):
    base_color_map[job] = adjust_lightness(palette[i % len(palette)], 1.15)

for job in job_roles:
    job_mask = grouped['Job_Role'] == job
    job_total = grouped.loc[job_mask, 'count'].sum()
    # compute how many in this job answered None (or left blank)
    none_mask = job_mask & (
        grouped['Mental_Health_Condition'].isna() |
        (grouped['Mental_Health_Condition'].astype(str).str.strip().str.lower() == 'none')
    )
    none_total = grouped.loc[none_mask, 'count'].sum()
    none_pct = (none_total / job_total) if job_total > 0 else 0
    # do not display 'None' percentage on the job label (user requested)
    labels.append(job)
    ids.append(job)
    parents.append("")
    values.append(job_total)
    colors.append(base_color_map[job])

    # get unique mental-health conditions for this job (may include 'None')
    mental_health_conditions = list(grouped.loc[job_mask, 'Mental_Health_Condition'].dropna().unique())
    for j, condition in enumerate(mental_health_conditions):
        cond_str = str(condition).strip()
        cond_l = cond_str.lower()
        # Skip creating a slice for 'None' answers (user requested not to display them)
        if cond_l == 'none' or cond_str == '' or pd.isna(condition):
            continue
        id_condition = f"{job} - {condition}"
        condition_mask = job_mask & (grouped['Mental_Health_Condition'] == condition)
        condition_total = grouped.loc[condition_mask, 'count'].sum()
        # percent of people in this job who reported this condition
        pct = (condition_total / job_total) if job_total > 0 else 0
        labels.append(f"{cond_str} ({pct*100:.1f}%)")
        ids.append(id_condition)
        parents.append(job)
        values.append(condition_total)
        factor = 0.95 + (j / max(1, len(mental_health_conditions))) * 0.1
        colors.append(adjust_lightness(base_color_map[job], factor))

        prods = list(grouped.loc[condition_mask, 'Productivity_Change'].dropna().unique())
        for k, prod in enumerate(prods):
            id_prod = f"{job} - {condition} - {prod}"
            prod_mask = condition_mask & (grouped['Productivity_Change'] == prod)
            prod_value = grouped.loc[prod_mask, 'count'].sum()
            labels.append(str(prod))
            ids.append(id_prod)
            parents.append(id_condition)
            values.append(prod_value)
            factor2 = 0.85 + (k / max(1, len(prods))) * 0.25
            colors.append(adjust_lightness(base_color_map[job], factor2))

# Option: set transform exponent to 1.0 to preserve exact proportions.
TRANSFORM_EXP = 1.0
original_values = values.copy()
plot_values = [ (v ** TRANSFORM_EXP) if (v is not None and v > 0) else 0 for v in original_values ]

# Compute percent-of-parent using original values (accurate percentages)
parent_sums = {}
for p, v in zip(parents, original_values):
    parent_sums[p] = parent_sums.get(p, 0) + (v if v is not None else 0)

# Build per-node text: show percent for Productivity_Change slices and
# append percent to specific mental-health condition labels
prod_pct_labels = {"increase", "decrease", "no change"}
mh_pct_labels = {"anxiety", "depression", "burnout"}
texts = []
customdata = []
for lab, par, orig in zip(labels, parents, original_values):
    pct = 0.0
    if par in parent_sums and parent_sums[par] > 0:
        pct = orig / parent_sums[par]
    lab_l = lab.strip().lower() if isinstance(lab, str) else ""
    if lab_l in prod_pct_labels:
        # show percent as separate text for productivity-change slices
        texts.append(f"{pct*100:.1f}%")
    else:
        # for mental-health condition slices we'll append percent into the label itself
        texts.append("")
    customdata.append([orig, f"{pct*100:.1f}%"])

# Append percent to mental-health condition labels (Anxiety/Depression/Burnout)
for i, lab in enumerate(labels):
    if isinstance(lab, str) and lab.strip().lower() in mh_pct_labels:
        pct_str = customdata[i][1]
        labels[i] = f"{lab} ({pct_str})"

# Create sunburst
fig = go.Figure(go.Sunburst(
    labels=labels,
    ids=ids,
    parents=parents,
    values=plot_values,
    customdata=customdata,
    text=texts,
    textinfo='label+text',
    insidetextorientation='radial',
    insidetextfont=dict(color='black', size=12),
    marker=dict(colors=colors, line=dict(color='#111111', width=2.5)),
    branchvalues='total',
    hovertemplate='<b>%{label}</b><br>Original value: %{customdata[0]}<br>Percent of parent: %{customdata[1]}<extra></extra>'
))

# Smaller layout for better visibility
fig.update_layout(
    title='Sunburst Chart of Remote Work Data',
    font=dict(color='black'),
    width=850, # Reduced from 1700
    height=600, # Reduced from 1200
    margin=dict(t=50, l=15, r=15, b=15),
)

# Display the plot directly in the notebook
fig.show()

"""**Interpretation**: The job with the most mental health condition is the Data Scientist, with 80.6% of
responders reporting a mental health condition. The highest percentage of any mental health condition
are the Data Scientists reporting Burnout (32.3%), with no change in productivity. The lowest percentage
are from Marketing who reported depression at the lowest rate of 20.9%. Only 68.9% of them reported a mental health condition. The other jobs all have reported mental health conditions at alarming percentages
in the 70+ percentages.
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression
import plotly.graph_objects as go


# 1. Fix Column Names for Compatibility

df = df.rename(columns={
    'Stress Level': 'Stress_Level',
    'Satisfaction with Remote Work': 'Satisfaction_with_Remote_Work',
    'Mental Health Condition': 'Mental_Health_Condition'
})

# ---------------------------
# 2. Define Key Columns
# ---------------------------
REMOTE_COL = "Satisfaction_with_Remote_Work"
MENTAL_COL = "Stress_Level"

# ---------------------------
# 3. Prepare Target Variable
# ---------------------------
target_raw = df[MENTAL_COL]

try:
    target_num = pd.to_numeric(target_raw, errors="coerce")
    is_numeric = target_num.notna().mean() > 0.5
except:
    is_numeric = False

if is_numeric:
    target = target_num.fillna(target_num.mean())
else:
    target = LabelEncoder().fit_transform(target_raw.fillna("MISSING"))

# ---------------------------
# 4. Mutual Information Scores
# ---------------------------
X = df.copy()
for col in X.columns:
    if X[col].dtype == object:
        X[col] = LabelEncoder().fit_transform(X[col].astype(str))
    else:
        X[col] = X[col].fillna(X[col].median())

if is_numeric:
    mi = mutual_info_regression(X, target)
else:
    mi = mutual_info_classif(X, target)

mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)

# ---------------------------
# 5. Correlation Heatmap
# ---------------------------
num_df = df.select_dtypes(include=[np.number])
corr = num_df.corr()

plt.figure(figsize=(10, 8))
plt.imshow(corr, cmap="viridis")
plt.xticks(range(len(corr.columns)), corr.columns, rotation=45, ha='right')
plt.yticks(range(len(corr.columns)), corr.columns)
plt.title("Correlation Heatmap")
plt.colorbar()
plt.tight_layout()
plt.show()

"""**Interpretation:** The heatmap shows that none of the variables are strongly correlated with each other, meaning age, experience, workload, virtual meetings, work–life balance, social isolation, and company support all behave independently. Because the correlations are close to zero, there are no multicollinearity concerns, and each variable contributes unique information to the analysis. This suggests that employees’ experiences vary individually and are not driven by a single demographic or workload factor."""

print(df.columns.tolist())

import pandas as pd

# 1. numeric columns
numeric_cols = [
    "Age",
    "Years_of_Experience",
    "Hours_Worked_Per_Week",
    "Number_of_Virtual_Meetings",
    "Work_Life_Balance_Rating",
    "Social_Isolation_Rating",
]

# 2. categorical columns
categorical_cols = [
    "Gender",
    "Job_Role",
    "Industry",
    "Work_Location",
    "Stress_Level",
    "Mental_Health_Condition",
    "Access_to_Mental_Health_Resources",
    "Productivity_Change",
    "Satisfaction_with_Remote_Work",
    "Company_Support_for_Remote_Work",
    "Physical_Activity",
]

# 3. agg dict
agg_dict = {}

for col in numeric_cols:
    agg_dict[col] = "mean"

for col in categorical_cols:
    agg_dict[col] = lambda x, c=col: x.value_counts().idxmax()

# 4. create region summary
region_summary = (
    df.groupby("Region")
      .agg(agg_dict)
      .reset_index()
)

# 5. round numeric
region_summary[numeric_cols] = region_summary[numeric_cols].round(2)

print(region_summary)

import matplotlib.pyplot as plt
import seaborn as sns


prod_counts = pd.crosstab(df["Region"], df["Productivity_Change"])

colors = ["#4C72B0", "#DD8452", "#55A868"]

plt.figure(figsize=(8,5))
ax = prod_counts.plot(kind="bar",
                      stacked=True,
                      figsize=(8,5),
                      color=colors)

plt.title("Productivity Change Distribution by Region")
plt.xlabel("Region")
plt.ylabel("Number of Employees")

for i, region in enumerate(prod_counts.index):
    cumulative = 0
    for j, change_type in enumerate(prod_counts.columns):
        value = prod_counts.loc[region, change_type]
        if value > 0:
            plt.text(
                i,
                cumulative + value/2,
                str(value),
                ha='center',
                va='center',
                color='white',
                fontsize=10,
                fontweight='bold'
            )
        cumulative += value
plt.legend(title="Productivity Change",
           bbox_to_anchor=(1.05, 1),
           loc="upper left")

plt.tight_layout()
plt.show()

"""**Interpretation:** Productivity has declined in the majority of regions, with Asia, Europe, North America, Oceania, and South America showing higher counts of employees experiencing decreased productivity compared to increases. Africa stands out as the only region where productivity increases exceed decreases, suggesting regional differences in remote-work adaptation or workforce conditions.

"""

## Heat Map of Work Location against stress levels for entire dataset MAM 113025 15:06

region_cond = pd.crosstab(
    df["Work_Location"],
    df["Stress_Level"]
)

custom_order = ["Low", "Medium", "High"]
df['Stress_Level'] = pd.Categorical(df['Stress_Level'], categories=custom_order, ordered=True)

plt.figure(figsize=(10, 6))
sns.heatmap(
    region_cond,
    annot=True,
    fmt="d",
    cmap="Purples"
)

plt.title("Counts of Stress Level by Work Location")
plt.xlabel("Stress Level")
plt.ylabel("Work Location")
plt.tight_layout()
plt.show()

"""This heat map is a clear visual of how remote work is a high stressor for the professionals surveyed for our dataset, across all regions, industries and jobs."""